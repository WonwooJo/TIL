{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konlpy 수행 중 java 오류 → 기존 java 삭제 → 최신버전 java 재설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# 토큰? 자연어 처리 대상(단위: 단어, 문장, 문단)\n",
    "# 토큰: 문법적으로 더이상 나눌 수 없는 요소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'After', 'preventing', 'President', 'Obama', \"'s\", 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', ',', 'Republicans', 'moved', 'swiftly', 'to', 'deliver', 'President', 'Trump', 'a', 'victory', 'days', 'before', 'the', 'election', '.', 'Both', 'presidential', 'candidates', 'campaigned', 'in', 'Pennsylvania', ',', 'where', 'Joe', 'Biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'Mr.', 'Trump', 'mocked', 'Kamala', 'Harris', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"Don't After preventing President Obama's from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'After', 'preventing', 'President', 'Obama', \"'\", 's', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', ',', 'Republicans', 'moved', 'swiftly', 'to', 'deliver', 'President', 'Trump', 'a', 'victory', 'days', 'before', 'the', 'election', '.', 'Both', 'presidential', 'candidates', 'campaigned', 'in', 'Pennsylvania', ',', 'where', 'Joe', 'Biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'Mr', '.', 'Trump', 'mocked', 'Kamala', 'Harris', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(\"Don't After preventing President Obama's from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "# text → word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'after', 'preventing', 'president', \"obama's\", 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', 'republicans', 'moved', 'swiftly', 'to', 'deliver', 'president', 'trump', 'a', 'victory', 'days', 'before', 'the', 'election', 'both', 'presidential', 'candidates', 'campaigned', 'in', 'pennsylvania', 'where', 'joe', 'biden', 'said', 'he', 'would', 'expand', 'his', 'electoral', 'map', 'and', 'mr', 'trump', 'mocked', 'kamala', 'harris']\n"
     ]
    }
   ],
   "source": [
    "print(text_to_word_sequence(\"Don't After preventing President Obama's from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# 여러개의 문장들을 한 문장씩 나누어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Don't After preventing President Ph.D. Obama's from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't After preventing President Ph.D. Obama's from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election.\", 'Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리나라 말이 어려운 이유\n",
    "# 1) 형태소\n",
    "#  - 자립형태소, 의존형태소\n",
    "# 2) 띄어쓰기: 아버지가방에들어가신다\n",
    "# 3) 품사에 따라 단어의 의미가 달리짐\n",
    "#  - 품사 태깅을 미리 수행하여 특정 단어가 사용되기 앞서 어떤 품사로 사용되는지 구분 필요\n",
    "#  - 품사 태깅 (tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'perventing', 'Ph.D.', 'Obama', 'from', 'filling', 'a', 'vacancy', 'four', 'years', 'ago', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"After perventing Ph.D. Obama from filling a vacancy four years ago.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('After', 'IN'),\n",
       " ('perventing', 'VBG'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('Obama', 'NNP'),\n",
       " ('from', 'IN'),\n",
       " ('filling', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('vacancy', 'NN'),\n",
       " ('four', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('ago', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=word_tokenize(text)\n",
    "from nltk.tag import pos_tag\n",
    "pos_tag(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '공부', '하고', '있는', '우리', '들', ',', '수료', '후', '에는', '취업', '에', '꼭', '성공해요']\n"
     ]
    }
   ],
   "source": [
    "okt=Okt()\n",
    "print(okt.morphs(\"열심히 공부하고 있는 우리들, 수료 후에는 취업에 꼭 성공해요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'Adverb'), ('공부', 'Noun'), ('하고', 'Josa'), ('있는', 'Adjective'), ('우리', 'Noun'), ('들', 'Suffix'), (',', 'Punctuation'), ('수료', 'Noun'), ('후', 'Noun'), ('에는', 'Josa'), ('취업', 'Noun'), ('에', 'Josa'), ('꼭', 'Noun'), ('성공해요', 'Adjective')]\n"
     ]
    }
   ],
   "source": [
    "# 형태소 단위로 품사 정보도 함께 출력\n",
    "print(okt.pos(\"열심히 공부하고 있는 우리들, 수료 후에는 취업에 꼭 성공해요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['공부', '우리', '수료', '후', '취업', '꼭']\n"
     ]
    }
   ],
   "source": [
    "# 품사가 명사인 것들만 추출\n",
    "print(okt.nouns(\"열심히 공부하고 있는 우리들, 수료 후에는 취업에 꼭 성공해요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['공부', '우리', '수료', '후', '취업', '성공']\n"
     ]
    }
   ],
   "source": [
    "km=Kkma()\n",
    "print(km.nouns(\"열심히 공부하고 있는 우리들, 수료 후에는 취업에 꼭 성공해요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 자연어 전처리, 언어 모델...\n",
    "# 2. 문서단어벡터(tfidf), 문서유사도\n",
    "# 3. 토픽모델링(lda)\n",
    "# 4. 머신러닝 기반 텍스트 분류\n",
    "# 5. RNN → LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제: 코퍼스(특정 도메인 단어 집합)로부터 노이즈 데이터 제거\n",
    "# ex) 인공지능 코퍼스(당뇨병)\n",
    "# 정규화: 동의어들 → 같은 단어로 통합\n",
    "# 불용어 제거\n",
    "# 빈도수가 낮은 단어 제거\n",
    "# 길이가 짧은 단어 제거\n",
    "\n",
    "# 정규표현식을 이용하여 작업하면 돼!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'were'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('were')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('watched','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[wnl.lemmatize(w,'v') for w in ['lives','going','doing','love']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['live', 'go', 'do', 'love']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출(stemming)\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "ls=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"After preventing President Obama's from filling a vacancy four years ago, Republicans moved swiftly to deliver President Trump a victory days before the election. Both presidential candidates campaigned in Pennsylvania, where Joe Biden said he would expand his electoral map and Mr. Trump mocked Kamala Harris.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'prevent', 'presid', 'obama', \"'s\", 'from', 'fill', 'a', 'vacanc', 'four', 'year', 'ago', ',', 'republican', 'move', 'swiftli', 'to', 'deliv', 'presid', 'trump', 'a', 'victori', 'day', 'befor', 'the', 'elect', '.', 'both', 'presidenti', 'candid', 'campaign', 'in', 'pennsylvania', ',', 'where', 'joe', 'biden', 'said', 'he', 'would', 'expand', 'hi', 'elector', 'map', 'and', 'mr.', 'trump', 'mock', 'kamala', 'harri', '.']\n"
     ]
    }
   ],
   "source": [
    "# 포터 알고리즘에 의한 어간 추출\n",
    "print([ps.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aft', 'prev', 'presid', 'obam', \"'s\", 'from', 'fil', 'a', 'vac', 'four', 'year', 'ago', ',', 'republ', 'mov', 'swift', 'to', 'del', 'presid', 'trump', 'a', 'vict', 'day', 'bef', 'the', 'elect', '.', 'both', 'presid', 'candid', 'campaign', 'in', 'pennsylvan', ',', 'wher', 'joe', 'bid', 'said', 'he', 'would', 'expand', 'his', 'elect', 'map', 'and', 'mr.', 'trump', 'mock', 'kamal', 'har', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ls.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간(stem), 어미(ending)\n",
    "# 어간: 긋다, 긋고... 잡다, 잡고, 잡기...\n",
    "# 어미: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어\n",
    "# nltk \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex=\"Family is not an important thing,It's everthing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english')) # set은 중복 X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt=word_tokenize(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family\n",
      "important\n",
      "thing\n",
      ",\n",
      "It\n",
      "'s\n",
      "everthing\n"
     ]
    }
   ],
   "source": [
    "for w in wt: # wt변수에 저장된 단어들을 하나씩 읽어 w에 저장\n",
    "    if w not in sw: # w가 sw에 존재하지 않는다면\n",
    "        print(w)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"텐서플로우와 머신러닝으로 자언어 처리를 하거든. 머신러닝으로만 자연어 처리를 하면 안돼.\"\n",
    "sw=\"어쨌든 아무거나 같다 비슷하다 하면 하거든 으로\" #불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=sw.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt=word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "for w in wt:\n",
    "    if w not in sw:\n",
    "        res.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['텐서플로우와', '머신러닝으로', '자언어', '처리를', '.', '머신러닝으로만', '자연어', '처리를', '안돼', '.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"전화번호: 010-1234-5678 주소: 서울시 강남구 나이: 28\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '5678', '28']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 숫자들만 추출\n",
    "re.findall(\"[0-9]+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"대한민국 한국 코리아 korea 남한 고려\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국 대한민국 대한민국 대한민국 대한민국 고려'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re.sub(\"정규표현식\", 치환문자(단어), 변수)\n",
    "re.sub(\"한국|코리아|korea|남한\",\"대한민국\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Her impact could be felt right away. There are major election disputes awaiting immediate action by the Supreme Court from the battleground states of North Carolina and Pennsylvania. Both concern the date by which absentee ballots may be accepted. With Justice Barrett’s elevation in place of Justice Ginsburg, a liberal icon, the court is expected to tilt decisively to the right. It is gaining a conservative who could sway cases in every area of American life, including abortion rights, gay rights, business regulation and the environment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 문장 토큰화\n",
    "text=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['impact', 'could', 'felt', 'right', 'away'], ['major', 'election', 'disputes', 'awaiting', 'immediate', 'action', 'supreme', 'court', 'battleground', 'states', 'north', 'carolina', 'pennsylvania'], ['concern', 'date', 'absentee', 'ballots', 'may', 'accepted'], ['justice', 'barrett', 'elevation', 'place', 'justice', 'ginsburg', 'liberal', 'icon', 'court', 'expected', 'tilt', 'decisively', 'right'], ['gaining', 'conservative', 'could', 'sway', 'cases', 'every', 'area', 'american', 'life', 'including', 'abortion', 'rights', 'gay', 'rights', 'business', 'regulation', 'environment']]\n"
     ]
    }
   ],
   "source": [
    "vocab={}\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "sentences=[]\n",
    "# 텍스트 cleaning\n",
    "# 문장 → 단어 토큰화 → 모두 문자화 → 불용어 제거 → 짧은 길이의 단어 제거\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i)\n",
    "    # print(sentence)\n",
    "    res=[]\n",
    "    for word in sentence:\n",
    "        word = word.lower() # 소문자화\n",
    "        if word not in stopWords:\n",
    "            if len(word) > 2:\n",
    "                res.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word]=0 # word가 ket, 0은 초기값  ex) {'her':0,'impact':0}\n",
    "                vocab[word] +=1 # {'her':1, 'impact':1}\n",
    "    sentences.append(res)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'impact': 1,\n",
       " 'could': 2,\n",
       " 'felt': 1,\n",
       " 'right': 2,\n",
       " 'away': 1,\n",
       " 'major': 1,\n",
       " 'election': 1,\n",
       " 'disputes': 1,\n",
       " 'awaiting': 1,\n",
       " 'immediate': 1,\n",
       " 'action': 1,\n",
       " 'supreme': 1,\n",
       " 'court': 2,\n",
       " 'battleground': 1,\n",
       " 'states': 1,\n",
       " 'north': 1,\n",
       " 'carolina': 1,\n",
       " 'pennsylvania': 1,\n",
       " 'concern': 1,\n",
       " 'date': 1,\n",
       " 'absentee': 1,\n",
       " 'ballots': 1,\n",
       " 'may': 1,\n",
       " 'accepted': 1,\n",
       " 'justice': 2,\n",
       " 'barrett': 1,\n",
       " 'elevation': 1,\n",
       " 'place': 1,\n",
       " 'ginsburg': 1,\n",
       " 'liberal': 1,\n",
       " 'icon': 1,\n",
       " 'expected': 1,\n",
       " 'tilt': 1,\n",
       " 'decisively': 1,\n",
       " 'gaining': 1,\n",
       " 'conservative': 1,\n",
       " 'sway': 1,\n",
       " 'cases': 1,\n",
       " 'every': 1,\n",
       " 'area': 1,\n",
       " 'american': 1,\n",
       " 'life': 1,\n",
       " 'including': 1,\n",
       " 'abortion': 1,\n",
       " 'rights': 2,\n",
       " 'gay': 1,\n",
       " 'business': 1,\n",
       " 'regulation': 1,\n",
       " 'environment': 1}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences # 코퍼스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "# tokenizer.fot_on_texts(코퍼스) → 코퍼스에 등장하는 단어들에 대한 빈도수를 기준으로 단어 집합 생성\n",
    "# 빈도수 낮은-높은 단어 순으로 번호를 부여 → 확인은 word_index로."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7,\n",
       " 'good': 8,\n",
       " 'knew': 9,\n",
       " 'driving': 10,\n",
       " 'crazy': 11,\n",
       " 'went': 12,\n",
       " 'mountain': 13}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('barber', 24),\n",
       "             ('person', 9),\n",
       "             ('good', 3),\n",
       "             ('huge', 15),\n",
       "             ('knew', 3),\n",
       "             ('secret', 18),\n",
       "             ('kept', 12),\n",
       "             ('word', 6),\n",
       "             ('keeping', 6),\n",
       "             ('driving', 3),\n",
       "             ('crazy', 3),\n",
       "             ('went', 3),\n",
       "             ('mountain', 3)])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs=5 # 가장 빈도수가 높은 5개 단어만 추출하겠다.\n",
    "tokenizer = Tokenizer(num_words=vs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('barber', 8),\n",
       "             ('person', 3),\n",
       "             ('good', 1),\n",
       "             ('huge', 5),\n",
       "             ('knew', 1),\n",
       "             ('secret', 6),\n",
       "             ('kept', 4),\n",
       "             ('word', 2),\n",
       "             ('keeping', 2),\n",
       "             ('driving', 1),\n",
       "             ('crazy', 1),\n",
       "             ('went', 1),\n",
       "             ('mountain', 1)])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7,\n",
       " 'good': 8,\n",
       " 'knew': 9,\n",
       " 'driving': 10,\n",
       " 'crazy': 11,\n",
       " 'went': 12,\n",
       " 'mountain': 13}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 5],\n",
       " [1, 3, 5],\n",
       " [2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4],\n",
       " [1, 4],\n",
       " [1, 4, 2],\n",
       " [3, 2, 1],\n",
       " [1, 3]]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
